import {
  useCallback,
  useMemo,
  useEffect,
  useState,
  startTransition,
} from "react";
import { useResearchStore } from "@/shared/stores/researchStore";
import { createProcessVoiceCommandUseCase } from "@/shared/useCases/ProcessVoiceCommandUseCase/factory";
import type { VoiceButtonState } from "../../types";

export function useVoiceRecognitionButtonViewModel() {
  const {
    isListening,
    setIsListening,
    setVoiceCommand,
    setPartialTranscript,
    clearPartialTranscript,
  } = useResearchStore();

  // ãƒã‚¤ãƒ‰ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã®çŠ¶æ…‹ç®¡ç†
  const [isMounted, setIsMounted] = useState(false);

  // UseCase ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
  const voiceUseCase = useMemo(() => {
    return createProcessVoiceCommandUseCase();
  }, []);

  // è‡ªå‹•åœæ­¢ã‚’ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡ï¼ˆæ—¢å®š: falseï¼‰
  const autoStop =
    (process.env.NEXT_PUBLIC_VOICE_AUTO_STOP || "").toLowerCase() === "1" ||
    (process.env.NEXT_PUBLIC_VOICE_AUTO_STOP || "").toLowerCase() === "true";

  // ãƒžã‚¦ãƒ³ãƒˆå¾Œã«çŠ¶æ…‹ã‚’æ›´æ–°
  useEffect(() => {
    setIsMounted(true);
  }, []);

  // ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ã¨ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ï¼ˆãƒã‚¤ãƒ‰ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
  const isSupported = useMemo(() => {
    // åˆå›žãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã¯falseã§çµ±ä¸€ã€ãƒžã‚¦ãƒ³ãƒˆå¾Œã«ãƒã‚§ãƒƒã‚¯
    if (!isMounted) return false;
    return voiceUseCase.checkSupport();
  }, [voiceUseCase, isMounted]);

  const hasPermission = true; // å®Ÿéš›ã®å®Ÿè£…ã§ã¯æ¨©é™ãƒã‚§ãƒƒã‚¯

  // ãƒ–ãƒ©ã‚¦ã‚¶ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚/æ¬¡ãƒ†ã‚£ãƒƒã‚¯ã«å‡¦ç†ã‚’å¾Œå›žã—
  const defer = useCallback((fn: () => void) => {
    type MaybeRIC = typeof globalThis & {
      requestIdleCallback?: (
        cb: () => void,
        opts?: { timeout?: number },
      ) => number;
    };
    const ric = (globalThis as MaybeRIC).requestIdleCallback;
    if (typeof ric === "function") {
      ric(() => fn(), { timeout: 200 });
    } else {
      setTimeout(fn, 0);
    }
  }, []);

  // ãƒœã‚¿ãƒ³çŠ¶æ…‹ã®è¨ˆç®—
  const buttonState: VoiceButtonState = useMemo(() => {
    const isDisabled = !isSupported || !hasPermission;

    return {
      text: isListening ? "åœæ­¢" : "éŸ³å£°èªè­˜é–‹å§‹",
      className: `px-4 py-2 rounded-md font-medium transition-colors ${
        isDisabled
          ? "bg-gray-400 text-gray-600 cursor-not-allowed"
          : isListening
            ? "bg-red-600 hover:bg-red-700 text-white"
            : "bg-green-600 hover:bg-green-700 text-white"
      }`,
      isDisabled,
      showIcon: !isListening,
    };
  }, [isListening, isSupported, hasPermission]);

  // éŸ³å£°èªè­˜ã®é–‹å§‹/åœæ­¢å‡¦ç†
  const handleToggleListening = useCallback(async () => {
    if (isListening) {
      try {
        if (!voiceUseCase.isProcessing) {
          setIsListening(false);
          return;
        }
        await voiceUseCase.stopProcessing();
        setIsListening(false);
        console.log("ðŸŽ™ï¸ éŸ³å£°èªè­˜åœæ­¢");
      } catch (error) {
        console.error("Failed to stop voice recognition:", error);
      }
    } else {
      try {
        console.log("ðŸŽ™ï¸ éŸ³å£°èªè­˜é–‹å§‹...");
        setIsListening(true);

        // AWS Transcribe ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
        const transcribeClient = voiceUseCase["transcribeClient"]; // ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        if (
          transcribeClient &&
          typeof transcribeClient.setEventHandlers === "function"
        ) {
          transcribeClient.setEventHandlers({
            onTranscriptionResult: (text: string, isFinal: boolean) => {
              console.log("ðŸŽ¯ éŸ³å£°èªè­˜çµæžœ:", text, "Final:", isFinal);

              if (isFinal) {
                // æœ€çµ‚çµæžœã¯å³æ™‚ã«UIã¸åæ˜ ï¼ˆåŒæœŸã‚³ã‚¹ãƒˆæœ€å°åŒ–ï¼‰
                startTransition(() => {
                  setVoiceCommand(text);
                  clearPartialTranscript();
                });

                // ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æžã¯ã‚¢ã‚¤ãƒ‰ãƒ«/æ¬¡ãƒ†ã‚£ãƒƒã‚¯ã¸å¾Œå›žã—ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è©°ã¾ã‚‰ã›ãªã„ï¼‰
                defer(() => {
                  const domainService = voiceUseCase["voiceDomainService"];
                  if (!domainService) return;
                  const parsedResult = domainService.parseVoiceCommand(text);
                  if (parsedResult.pattern && parsedResult.confidence > 0.5) {
                    console.log(
                      "âœ… éŸ³å£°ã‚³ãƒžãƒ³ãƒ‰èªè­˜æˆåŠŸ:",
                      text,
                      "ãƒ‘ã‚¿ãƒ¼ãƒ³:",
                      parsedResult.pattern,
                    );
                    if (autoStop) {
                      setIsListening(false);
                      if (voiceUseCase.isProcessing) {
                        voiceUseCase
                          .stopProcessing()
                          .catch((err) =>
                            console.error(
                              "Failed to stop after recognition:",
                              err,
                            ),
                          );
                      }
                    }
                  }
                });
              } else {
                console.log("ðŸ“ é€”ä¸­çµæžœ:", text);
                setPartialTranscript(text);
              }
            },
            onError: (error) => {
              console.error("AWS Transcribe Error:", error);
              const isSilenceTimeout =
                error.error === "network" &&
                typeof error.message === "string" &&
                error.message.toLowerCase().includes("no audio activity");

              if (isSilenceTimeout && isListening) {
                // è‡ªå‹•å†æŽ¥ç¶šï¼ˆUIã¯ç¶™ç¶šçŠ¶æ…‹ã®ã¾ã¾ï¼‰
                voiceUseCase
                  .stopProcessing()
                  .catch(() => void 0)
                  .then(() => voiceUseCase.startRealTimeTranscription())
                  .catch((err) => {
                    console.error("Auto-reconnect failed:", err);
                    setIsListening(false);
                  });
              } else {
                setIsListening(false);
                // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                if (error.error === "transcription-failed") {
                  console.warn("ðŸš¨ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:", error.message);
                } else if (error.error === "not-allowed") {
                  console.warn("ðŸš¨ ãƒžã‚¤ã‚¯æ¨©é™ã‚¨ãƒ©ãƒ¼:", error.message);
                } else {
                  console.warn("ðŸš¨ éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚¨ãƒ©ãƒ¼:", error.message);
                }
              }
            },
            onConnectionStatusChange: (status) => {
              console.log("ðŸ”— æŽ¥ç¶šçŠ¶æ…‹å¤‰æ›´:", status);
            },
          });
        }

        // éŸ³å£°èªè­˜ã‚’é–‹å§‹ï¼ˆUIã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
        voiceUseCase.startRealTimeTranscription().catch((err) => {
          console.error("Failed to start voice recognition:", err);
          setIsListening(false);
          clearPartialTranscript();
        });
      } catch (error) {
        console.error("Failed to start voice recognition:", error);
        setIsListening(false);
        clearPartialTranscript();
      }
    }
  }, [
    isListening,
    voiceUseCase,
    setIsListening,
    setVoiceCommand,
    autoStop,
    clearPartialTranscript,
    defer,
    setPartialTranscript,
  ]);

  // æ¨©é™è¦æ±‚
  const requestPermission = useCallback(async () => {
    try {
      return await voiceUseCase.requestPermission();
    } catch (error) {
      console.error("Failed to request microphone permission:", error);
      return false;
    }
  }, [voiceUseCase]);

  return {
    buttonState,
    isSupported,
    hasPermission,
    handleToggleListening,
    requestPermission,
  };
}
